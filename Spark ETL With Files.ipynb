{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ETL with Files (CSV/JSON/Parquet/Text)\n",
    "\n",
    "Reference Notebook: \n",
    "\n",
    "Reference: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required library\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kok\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pyspark\\context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Start Spaek Ssssion\n",
    "spark = SparkSession.builder.appName(\"ETL with files\").getOrCreate()\n",
    "\n",
    "sqlContext = SparkSession(spark)\n",
    "\n",
    "#Do not show warnings, only show errors\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below reads CSV into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the CSV into a Spark DataFrame\n",
    "csvdf = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"nyc_taxi_zone.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking the dataframe schema\n",
    "csvdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#View the first 10 rows of the CSV\n",
    "csvdf.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code below reads a JSON file into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the JSON file into a Spark DataFrame\n",
    "jsondf = spark.read.format(\"json\").option(\"multiline\",\"true\").load(\"nyc_taxi_zone.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- LocationID: long (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsondf.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The code below reads a Parquet file into a Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Parquet file into a Spark DataFrame\n",
    "parquetdf = spark.read.format(\"parquet\").load(\"yellow_tripdata_2024-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2964624"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquetdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reads a JSON file into a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading a textfile with no options\n",
    "textdf =spark.read.text(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|id,name,Fare|\n",
      "|  1,Adam,1.5|\n",
      "|   2,Bob,1.8|\n",
      "| 3,Chris,2.1|\n",
      "|4,Daniel,0.7|\n",
      "| 5,Ethan,3.9|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf.show()\n",
    "#The df lumps all columns into a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf2 = spark.read.option(\"lineSep\",\",\").text(\"sample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|     id|\n",
      "|   name|\n",
      "|Fare\\n1|\n",
      "|   Adam|\n",
      "| 1.5\\n2|\n",
      "|    Bob|\n",
      "| 1.8\\n3|\n",
      "|  Chris|\n",
      "| 2.1\\n4|\n",
      "| Daniel|\n",
      "| 0.7\\n5|\n",
      "|  Ethan|\n",
      "|    3.9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Temp table in Spark Session for the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdf.createOrReplaceTempView(\"tempCSV_table\")\n",
    "jsondf.createOrReplaceTempView(\"tempJSON_table\")\n",
    "parquetdf.createOrReplaceTempView(\"tempParquet_table\")\n",
    "textdf2.createOrReplaceTempView(\"tempTxt_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM tempCSV_table LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+------------+\n",
      "|      Borough|LocationID|                Zone|service_zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "|          EWR|         1|      Newark Airport|         EWR|\n",
      "|       Queens|         2|         Jamaica Bay|   Boro Zone|\n",
      "|        Bronx|         3|Allerton/Pelham G...|   Boro Zone|\n",
      "|    Manhattan|         4|       Alphabet City| Yellow Zone|\n",
      "|Staten Island|         5|       Arden Heights|   Boro Zone|\n",
      "|Staten Island|         6|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|       Queens|         7|             Astoria|   Boro Zone|\n",
      "|       Queens|         8|        Astoria Park|   Boro Zone|\n",
      "|       Queens|         9|          Auburndale|   Boro Zone|\n",
      "|       Queens|        10|        Baisley Park|   Boro Zone|\n",
      "+-------------+----------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM tempJSON_table LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM tempParquet_table LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
